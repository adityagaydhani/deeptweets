{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPT-2 implementation is adapted from the HuggingFace library: https://huggingface.co/gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GlLlFGsS7g0i"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madityagaydhani\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qaSI1NZtMn3G"
   },
   "outputs": [],
   "source": [
    "handle = 'realDonaldTrump' # Change handle to JoeBiden for training the model on Joe Biden's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pw3L46xzNFGV"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'../../data/{handle}.csv')\n",
    "my_tweets = df['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LIutO2HNvuRg"
   },
   "outputs": [],
   "source": [
    "def make_dataset(dataset, epochs):\n",
    "    r = Rake(max_length=2)\n",
    "    total_text = '<|endoftext|>'\n",
    "    tweets = [t for t in dataset]\n",
    "    for _ in range(epochs):\n",
    "        random.shuffle(tweets)\n",
    "        for t in tweets:\n",
    "            r.extract_keywords_from_text(t)\n",
    "            context = ' '.join(r.get_ranked_phrases())\n",
    "            total_text += context + '<|endofcontext|>'\n",
    "            total_text += t + '<|endoftext|>'\n",
    "    return total_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4OjOW_4x7g1H"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 4\n",
    "\n",
    "with open(f'../../data/{handle}_context_train.txt', 'w') as f:\n",
    "    data = make_dataset(my_tweets, EPOCHS)\n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X4LWV56z7g1Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-20 15:15:48.883703: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "12/20/2020 15:15:51 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "12/20/2020 15:15:51 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='output/realDonaldTrump_context', overwrite_output_dir=True, do_train=True, do_eval=False, do_predict=False, model_parallel=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=1, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec20_15-15-51_aditya-XPS-15-9570', logging_first_step=False, logging_steps=20, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=20, dataloader_num_workers=0, past_index=-1, run_name='output/realDonaldTrump_context', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fp16_backend='auto', sharded_ddp=False)\n",
      "[INFO|configuration_utils.py:431] 2020-12-20 15:15:52,132 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /home/aditya/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "[INFO|configuration_utils.py:467] 2020-12-20 15:15:52,133 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:431] 2020-12-20 15:15:52,272 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /home/aditya/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "[INFO|configuration_utils.py:467] 2020-12-20 15:15:52,274 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1802] 2020-12-20 15:15:52,738 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /home/aditya/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "[INFO|tokenization_utils_base.py:1802] 2020-12-20 15:15:52,739 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /home/aditya/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1802] 2020-12-20 15:15:52,739 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /home/aditya/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "/media/aditya/Data/MS/Fall 2020/CSCI 5525/Project/deep_tweets_workspace/lib/python3.6/site-packages/transformers/models/auto/modeling_auto.py:890: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "[INFO|modeling_utils.py:1024] 2020-12-20 15:15:52,987 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /home/aditya/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
      "[INFO|modeling_utils.py:1140] 2020-12-20 15:15:56,900 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:1149] 2020-12-20 15:15:56,900 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "/media/aditya/Data/MS/Fall 2020/CSCI 5525/Project/deep_tweets_workspace/lib/python3.6/site-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n",
      "12/20/2020 15:15:56 - INFO - filelock -   Lock 139888164311280 acquired on ../../data/cached_lm_GPT2TokenizerFast_1024_realDonaldTrump_context_train.txt.lock\n",
      "[INFO|language_modeling.py:88] 2020-12-20 15:15:56,902 >> Creating features from dataset file at ../../data\n",
      "[WARNING|tokenization_utils_base.py:3233] 2020-12-20 15:15:57,732 >> Token indices sequence length is longer than the specified maximum sequence length for this model (413237 > 1024). Running this sequence through the model will result in indexing errors\n",
      "[INFO|language_modeling.py:108] 2020-12-20 15:15:58,071 >> Saving features into cached file ../../data/cached_lm_GPT2TokenizerFast_1024_realDonaldTrump_context_train.txt [took 0.011 s]\n",
      "12/20/2020 15:15:58 - INFO - filelock -   Lock 139888164311280 released on ../../data/cached_lm_GPT2TokenizerFast_1024_realDonaldTrump_context_train.txt.lock\n",
      "[WARNING|training_args.py:423] 2020-12-20 15:15:58,076 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:423] 2020-12-20 15:15:58,079 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|trainer.py:703] 2020-12-20 15:15:58,079 >> ***** Running training *****\n",
      "[INFO|trainer.py:704] 2020-12-20 15:15:58,079 >>   Num examples = 403\n",
      "[INFO|trainer.py:705] 2020-12-20 15:15:58,079 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:706] 2020-12-20 15:15:58,079 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:707] 2020-12-20 15:15:58,079 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:708] 2020-12-20 15:15:58,079 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:709] 2020-12-20 15:15:58,079 >>   Total optimization steps = 403\n",
      "[WARNING|training_args.py:423] 2020-12-20 15:15:58,083 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "output/realDonaldTrump_context\n",
      "[INFO|integrations.py:371] 2020-12-20 15:15:58,098 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "[WARNING|training_args.py:423] 2020-12-20 15:15:58,098 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madityagaydhani\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33moutput/realDonaldTrump_context\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/adityagaydhani/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/adityagaydhani/huggingface/runs/1kka1nz8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /media/aditya/Data/MS/Fall 2020/CSCI 5525/Project/deep_tweets_workspace/deeptweets/models/GPT2/wandb/run-20201220_151558-1kka1nz8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "  5%|‚ñç         | 20/403 [01:43<33:08,  5.19s/it]{'loss': 4.259286117553711, 'learning_rate': 4.751861042183623e-05, 'epoch': 0.04962779156327544}\n",
      " 10%|‚ñâ         | 40/403 [03:25<32:14,  5.33s/it]{'loss': 3.8039676666259767, 'learning_rate': 4.5037220843672454e-05, 'epoch': 0.09925558312655088}\n",
      " 15%|‚ñà‚ñç        | 60/403 [05:10<28:54,  5.06s/it]{'loss': 3.572294998168945, 'learning_rate': 4.2555831265508686e-05, 'epoch': 0.1488833746898263}\n",
      " 20%|‚ñà‚ñâ        | 80/403 [06:49<25:57,  4.82s/it]{'loss': 3.4690513610839844, 'learning_rate': 4.007444168734492e-05, 'epoch': 0.19851116625310175}\n",
      " 25%|‚ñà‚ñà‚ñç       | 100/403 [08:29<29:05,  5.76s/it]{'loss': 3.444041061401367, 'learning_rate': 3.7593052109181144e-05, 'epoch': 0.24813895781637718}\n",
      " 30%|‚ñà‚ñà‚ñâ       | 120/403 [10:15<24:29,  5.19s/it]{'loss': 3.2796886444091795, 'learning_rate': 3.511166253101737e-05, 'epoch': 0.2977667493796526}\n",
      " 31%|‚ñà‚ñà‚ñà       | 123/403 [10:30<24:13,  5.19s/it]"
     ]
    }
   ],
   "source": [
    "!python ../../scripts/run_language_modeling.py \\\n",
    "    --output_dir=output/$handle\\_context \\\n",
    "    --overwrite_output_dir \\\n",
    "    --overwrite_cache \\\n",
    "    --model_type=gpt2 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --do_train --train_data_file=../../data/$handle\\_context_train.txt \\\n",
    "    --logging_steps 20 \\\n",
    "    --per_gpu_train_batch_size 1 \\\n",
    "    --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "viSAJ7EE7g1T"
   },
   "source": [
    "## Generate new tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uQRNMedK7g1T"
   },
   "outputs": [],
   "source": [
    "r = Rake(max_length=2)\n",
    "\n",
    "# Replace this tweet with any other tweet.\n",
    "tweet_context = \"The drug companies are going crazy putting up nasty ads against me asking to ‚Äúwithdraw my Favored Nation‚Äôs Executive Order‚Äù. They don‚Äôt want the U.S. to have the lowest drug prices in the world, but we now will. Big reductions coming. No other politician would do this!!!\"\n",
    "\n",
    "r.extract_keywords_from_text(tweet_context)\n",
    "SENTENCES = [' '.join(r.get_ranked_phrases())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EkD5CRkn7g1Y"
   },
   "outputs": [],
   "source": [
    "seed = random.randint(0, 2**32-1)\n",
    "examples = []\n",
    "num_return_sequences = 3\n",
    "\n",
    "for start in SENTENCES:\n",
    "    val = !python ../../run_generation.py \\\n",
    "        --model_type gpt2 \\\n",
    "        --model_name_or_path output/$handle\\_context \\\n",
    "        --length 160 \\\n",
    "        --num_return_sequences $num_return_sequences \\\n",
    "        --temperature 1 \\\n",
    "        --p 0.95 \\\n",
    "        --seed $seed \\\n",
    "        --prompt {'\"<|endoftext|>' + start + '<|endofcontext|>\"'}\n",
    "    generated = [val[-2*(k+1)] for k in range(num_return_sequences)[::-1]]\n",
    "    print(f'\\nStart of sentence: {start}')\n",
    "    for i, g in enumerate(generated):\n",
    "        g = g.split('<|endofcontext|>', 1)[1]\n",
    "        g = g.replace('<|endoftext|>', '')\n",
    "        g = g.replace('<|endofcontext|>', '')\n",
    "        print(f'* Generated #{i+1}: {g}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "huggingtweets-dev.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
