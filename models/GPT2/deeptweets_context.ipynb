{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPT-2 implementation is adapted from the HuggingFace library: https://huggingface.co/gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GlLlFGsS7g0i"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madityagaydhani\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qaSI1NZtMn3G"
   },
   "outputs": [],
   "source": [
    "handle = 'realDonaldTrump' # Change handle to JoeBiden for training the model on Joe Biden's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pw3L46xzNFGV"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'../../data/{handle}.csv')\n",
    "my_tweets = df['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LIutO2HNvuRg"
   },
   "outputs": [],
   "source": [
    "def make_dataset(dataset, epochs):\n",
    "    r = Rake(max_length=2)\n",
    "    total_text = '<|endoftext|>'\n",
    "    tweets = [t for t in dataset]\n",
    "    for _ in range(epochs):\n",
    "        random.shuffle(tweets)\n",
    "        for t in tweets:\n",
    "            r.extract_keywords_from_text(t)\n",
    "            context = ' '.join(r.get_ranked_phrases())\n",
    "            total_text += context + '<|endofcontext|>'\n",
    "            total_text += t + '<|endoftext|>'\n",
    "    return total_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4OjOW_4x7g1H"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 4\n",
    "\n",
    "with open(f'../../data/{handle}_context_train.txt', 'w') as f:\n",
    "    data = make_dataset(my_tweets, EPOCHS)\n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X4LWV56z7g1Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-20 15:15:48.883703: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "12/20/2020 15:15:51 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "12/20/2020 15:15:51 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='output/realDonaldTrump_context', overwrite_output_dir=True, do_train=True, do_eval=False, do_predict=False, model_parallel=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=1, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec20_15-15-51_aditya-XPS-15-9570', logging_first_step=False, logging_steps=20, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=20, dataloader_num_workers=0, past_index=-1, run_name='output/realDonaldTrump_context', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fp16_backend='auto', sharded_ddp=False)\n",
      "[INFO|configuration_utils.py:431] 2020-12-20 15:15:52,132 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /home/aditya/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "[INFO|configuration_utils.py:467] 2020-12-20 15:15:52,133 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:431] 2020-12-20 15:15:52,272 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /home/aditya/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "[INFO|configuration_utils.py:467] 2020-12-20 15:15:52,274 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1802] 2020-12-20 15:15:52,738 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /home/aditya/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "[INFO|tokenization_utils_base.py:1802] 2020-12-20 15:15:52,739 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /home/aditya/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1802] 2020-12-20 15:15:52,739 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /home/aditya/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "/media/aditya/Data/MS/Fall 2020/CSCI 5525/Project/deep_tweets_workspace/lib/python3.6/site-packages/transformers/models/auto/modeling_auto.py:890: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "[INFO|modeling_utils.py:1024] 2020-12-20 15:15:52,987 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /home/aditya/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
      "[INFO|modeling_utils.py:1140] 2020-12-20 15:15:56,900 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:1149] 2020-12-20 15:15:56,900 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "/media/aditya/Data/MS/Fall 2020/CSCI 5525/Project/deep_tweets_workspace/lib/python3.6/site-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n",
      "12/20/2020 15:15:56 - INFO - filelock -   Lock 139888164311280 acquired on ../../data/cached_lm_GPT2TokenizerFast_1024_realDonaldTrump_context_train.txt.lock\n",
      "[INFO|language_modeling.py:88] 2020-12-20 15:15:56,902 >> Creating features from dataset file at ../../data\n",
      "[WARNING|tokenization_utils_base.py:3233] 2020-12-20 15:15:57,732 >> Token indices sequence length is longer than the specified maximum sequence length for this model (413237 > 1024). Running this sequence through the model will result in indexing errors\n",
      "[INFO|language_modeling.py:108] 2020-12-20 15:15:58,071 >> Saving features into cached file ../../data/cached_lm_GPT2TokenizerFast_1024_realDonaldTrump_context_train.txt [took 0.011 s]\n",
      "12/20/2020 15:15:58 - INFO - filelock -   Lock 139888164311280 released on ../../data/cached_lm_GPT2TokenizerFast_1024_realDonaldTrump_context_train.txt.lock\n",
      "[WARNING|training_args.py:423] 2020-12-20 15:15:58,076 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:423] 2020-12-20 15:15:58,079 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|trainer.py:703] 2020-12-20 15:15:58,079 >> ***** Running training *****\n",
      "[INFO|trainer.py:704] 2020-12-20 15:15:58,079 >>   Num examples = 403\n",
      "[INFO|trainer.py:705] 2020-12-20 15:15:58,079 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:706] 2020-12-20 15:15:58,079 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:707] 2020-12-20 15:15:58,079 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:708] 2020-12-20 15:15:58,079 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:709] 2020-12-20 15:15:58,079 >>   Total optimization steps = 403\n",
      "[WARNING|training_args.py:423] 2020-12-20 15:15:58,083 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "output/realDonaldTrump_context\n",
      "[INFO|integrations.py:371] 2020-12-20 15:15:58,098 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "[WARNING|training_args.py:423] 2020-12-20 15:15:58,098 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madityagaydhani\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33moutput/realDonaldTrump_context\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/adityagaydhani/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/adityagaydhani/huggingface/runs/1kka1nz8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /media/aditya/Data/MS/Fall 2020/CSCI 5525/Project/deep_tweets_workspace/deeptweets/models/GPT2/wandb/run-20201220_151558-1kka1nz8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "  5%|▍         | 20/403 [01:43<33:08,  5.19s/it]{'loss': 4.259286117553711, 'learning_rate': 4.751861042183623e-05, 'epoch': 0.04962779156327544}\n",
      " 10%|▉         | 40/403 [03:25<32:14,  5.33s/it]{'loss': 3.8039676666259767, 'learning_rate': 4.5037220843672454e-05, 'epoch': 0.09925558312655088}\n",
      " 15%|█▍        | 60/403 [05:10<28:54,  5.06s/it]{'loss': 3.572294998168945, 'learning_rate': 4.2555831265508686e-05, 'epoch': 0.1488833746898263}\n",
      " 20%|█▉        | 80/403 [06:49<25:57,  4.82s/it]{'loss': 3.4690513610839844, 'learning_rate': 4.007444168734492e-05, 'epoch': 0.19851116625310175}\n",
      " 25%|██▍       | 100/403 [08:29<29:05,  5.76s/it]{'loss': 3.444041061401367, 'learning_rate': 3.7593052109181144e-05, 'epoch': 0.24813895781637718}\n",
      " 30%|██▉       | 120/403 [10:15<24:29,  5.19s/it]{'loss': 3.2796886444091795, 'learning_rate': 3.511166253101737e-05, 'epoch': 0.2977667493796526}\n",
      "                                                 {'loss': 3.314830017089844, 'learning_rate': 3.26302729528536e-05, 'epoch': 0.34739454094292804}\n",
      " 40%|███▉      | 160/403 [13:50<24:08,  5.96s/it]14888337468983e-05, 'epoch': 0.3970223325062035}\n",
      " 45%|████▍     | 180/403 [15:32<18:45,  5.05s/it]{'loss': 3.1468162536621094, 'learning_rate': 2.7667493796526056e-05, 'epoch': 0.4466501240694789}\n",
      " 50%|████▉     | 200/403 [17:15<18:48,  5.56s/it]{'loss': 3.162366485595703, 'learning_rate': 2.5186104218362282e-05, 'epoch': 0.49627791563275436}\n",
      " 55%|█████▍    | 220/403 [18:52<14:46,  4.84s/it]{'loss': 3.0874237060546874, 'learning_rate': 2.2704714640198514e-05, 'epoch': 0.5459057071960298}\n",
      " 60%|█████▉    | 240/403 [20:29<13:12,  4.86s/it]{'loss': 3.1037689208984376, 'learning_rate': 2.022332506203474e-05, 'epoch': 0.5955334987593052}\n",
      " 65%|██████▍   | 260/403 [22:06<11:37,  4.88s/it]74193548387097e-05, 'epoch': 0.6451612903225806}\n",
      " 69%|██████▉   | 280/403 [23:48<10:12,  4.98s/it]{'loss': 3.0930423736572266, 'learning_rate': 1.5260545905707198e-05, 'epoch': 0.6947890818858561}\n",
      " 74%|███████▍  | 300/403 [25:29<09:23,  5.47s/it]{'loss': 3.1016096115112304, 'learning_rate': 1.2779156327543427e-05, 'epoch': 0.7444168734491315}\n",
      " 79%|███████▉  | 320/403 [27:07<06:51,  4.96s/it]{'loss': 3.0308483123779295, 'learning_rate': 1.0297766749379652e-05, 'epoch': 0.794044665012407}\n",
      " 84%|████████▍ | 340/403 [28:44<05:06,  4.87s/it]{'loss': 3.000766944885254, 'learning_rate': 7.816377171215881e-06, 'epoch': 0.8436724565756824}\n",
      " 89%|████████▉ | 360/403 [30:21<03:30,  4.89s/it]{'loss': 2.9473012924194335, 'learning_rate': 5.334987593052109e-06, 'epoch': 0.8933002481389578}\n",
      " 94%|█████████▍| 380/403 [31:59<01:57,  5.09s/it]{'loss': 2.995980644226074, 'learning_rate': 2.8535980148883375e-06, 'epoch': 0.9429280397022333}\n",
      " 99%|█████████▉| 400/403 [33:41<00:16,  5.53s/it]{'loss': 2.98781681060791, 'learning_rate': 3.7220843672456576e-07, 'epoch': 0.9925558312655087}\n",
      "100%|██████████| 403/403 [33:55<00:00,  5.10s/it][INFO|trainer.py:862] 2020-12-20 15:49:55,828 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 403/403 [33:55<00:00,  5.10s/it]{'epoch': 1.0}\n",
      "100%|██████████| 403/403 [33:55<00:00,  5.05s/it]\n",
      "[INFO|trainer.py:1226] 2020-12-20 15:49:55,831 >> Saving model checkpoint to output/realDonaldTrump_context\n",
      "[INFO|configuration_utils.py:289] 2020-12-20 15:49:55,834 >> Configuration saved in output/realDonaldTrump_context/config.json\n",
      "[INFO|modeling_utils.py:814] 2020-12-20 15:49:58,264 >> Model weights saved in output/realDonaldTrump_context/pytorch_model.bin\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 6003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /media/aditya/Data/MS/Fall 2020/CSCI 5525/Project/deep_tweets_workspace/deeptweets/models/GPT2/wandb/run-20201220_151558-1kka1nz8/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /media/aditya/Data/MS/Fall 2020/CSCI 5525/Project/deep_tweets_workspace/deeptweets/models/GPT2/wandb/run-20201220_151558-1kka1nz8/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                      train/loss 2.98782\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                     train/epoch 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                           _step 403\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                        _runtime 2040\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                      _timestamp 1608500998\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                train/total_flos 308116946681856\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▆▄▄▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/learning_rate ██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▆▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▆▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33moutput/realDonaldTrump_context\u001b[0m: \u001b[34mhttps://wandb.ai/adityagaydhani/huggingface/runs/1kka1nz8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python ../../scripts/run_language_modeling.py \\\n",
    "    --output_dir=output/$handle\\_context \\\n",
    "    --overwrite_output_dir \\\n",
    "    --overwrite_cache \\\n",
    "    --model_type=gpt2 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --do_train --train_data_file=../../data/$handle\\_context_train.txt \\\n",
    "    --logging_steps 20 \\\n",
    "    --per_gpu_train_batch_size 1 \\\n",
    "    --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "viSAJ7EE7g1T"
   },
   "source": [
    "## Generate new tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uQRNMedK7g1T"
   },
   "outputs": [],
   "source": [
    "r = Rake(max_length=2)\n",
    "\n",
    "# Replace this tweet with any other tweet.\n",
    "tweet_context = \"The drug companies are going crazy putting up nasty ads against me asking to “withdraw my Favored Nation’s Executive Order”. They don’t want the U.S. to have the lowest drug prices in the world, but we now will. Big reductions coming. No other politician would do this!!!\"\n",
    "\n",
    "r.extract_keywords_from_text(tweet_context)\n",
    "SENTENCES = [' '.join(r.get_ranked_phrases())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EkD5CRkn7g1Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context: The drug companies are going crazy putting up nasty ads against me asking to “withdraw my Favored Nation’s Executive Order”. They don’t want the U.S. to have the lowest drug prices in the world, but we now will. Big reductions coming. No other politician would do this!!!\n",
      "* Generated #1: “Even today, I heard some nasty attacks from a corrupt politician who wants to withdraw the whole “Volunteer” support..”\n",
      "* Generated #2: I “ve received much lower prices and dangerous ads. The American people want “take care of their congressman”, asking for U.S. drug companies to turn over their “dide to drug companies. Doing nothing was nasty for our Country! — Senator Joe Biden (@SenatorBiden) November 4, 2016\n",
      "* Generated #3: “deport our corrupt politician the Drug Companies—“to Drain The Swamp!\n"
     ]
    }
   ],
   "source": [
    "seed = random.randint(0, 2**32-1)\n",
    "examples = []\n",
    "num_return_sequences = 3\n",
    "\n",
    "for start in SENTENCES:\n",
    "    val = !python ../../scripts/run_generation.py \\\n",
    "        --model_type gpt2 \\\n",
    "        --model_name_or_path output/$handle\\_context \\\n",
    "        --length 160 \\\n",
    "        --num_return_sequences $num_return_sequences \\\n",
    "        --temperature 1 \\\n",
    "        --p 0.95 \\\n",
    "        --seed $seed \\\n",
    "        --prompt {'\"<|endoftext|>' + start + '<|endofcontext|>\"'}\n",
    "    generated = [val[-2*(k+1)] for k in range(num_return_sequences)[::-1]]\n",
    "    print(f'\\nContext: {tweet_context}')\n",
    "    for i, g in enumerate(generated):\n",
    "        g = g.split('<|endofcontext|>', 1)[1]\n",
    "        g = g.replace('<|endoftext|>', '')\n",
    "        g = g.replace('<|endofcontext|>', '')\n",
    "        print(f'* Generated #{i+1}: {g}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "huggingtweets-dev.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
