{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPT-2 implementation is adapted from the HuggingFace library: https://huggingface.co/gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GlLlFGsS7g0i"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qaSI1NZtMn3G"
   },
   "outputs": [],
   "source": [
    "handle = 'realDonaldTrump' # Change handle to JoeBiden for training the model on Joe Biden's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pw3L46xzNFGV"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'../../data/{handle}.csv')\n",
    "my_tweets = df['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LIutO2HNvuRg"
   },
   "outputs": [],
   "source": [
    "def make_dataset(dataset, epochs):\n",
    "    total_text = '<|endoftext|>'\n",
    "    tweets = [t for t in dataset]\n",
    "    for _ in range(epochs):\n",
    "        random.shuffle(tweets)\n",
    "        total_text += '<|endoftext|>'.join(tweets) + '<|endoftext|>'\n",
    "    return total_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4OjOW_4x7g1H"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 4\n",
    "\n",
    "with open(f'../../data/{handle}_train.txt', 'w') as f:\n",
    "    data = make_dataset(my_tweets, EPOCHS)\n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X4LWV56z7g1Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/20/2020 13:20:18 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "12/20/2020 13:20:18 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='output/realDonaldTrump', overwrite_output_dir=True, do_train=True, do_eval=False, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=1, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec20_13-20-18_aditya-XPS-15-9570', logging_first_step=False, logging_steps=20, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=20, dataloader_num_workers=0, past_index=-1, run_name='output/realDonaldTrump', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n",
      "[INFO|configuration_utils.py:413] 2020-12-20 13:20:19,131 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /home/aditya/.cache/torch/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "[INFO|configuration_utils.py:449] 2020-12-20 13:20:19,133 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:413] 2020-12-20 13:20:19,279 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /home/aditya/.cache/torch/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "[INFO|configuration_utils.py:449] 2020-12-20 13:20:19,281 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1650] 2020-12-20 13:20:19,585 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /home/aditya/.cache/torch/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "[INFO|tokenization_utils_base.py:1650] 2020-12-20 13:20:19,586 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /home/aditya/.cache/torch/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "/home/aditya/my_mm_workspace/lib/python3.6/site-packages/transformers/modeling_auto.py:837: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "[INFO|modeling_utils.py:940] 2020-12-20 13:20:19,860 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /home/aditya/.cache/torch/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
      "[INFO|modeling_utils.py:1056] 2020-12-20 13:20:23,493 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:1065] 2020-12-20 13:20:23,493 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "/home/aditya/my_mm_workspace/lib/python3.6/site-packages/transformers/data/datasets/language_modeling.py:44: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n",
      "12/20/2020 13:20:23 - INFO - filelock -   Lock 140558156480976 acquired on ../../data/cached_lm_GPT2Tokenizer_1024_realDonaldTrump_train.txt.lock\n",
      "[INFO|language_modeling.py:74] 2020-12-20 13:20:23,494 >> Creating features from dataset file at ../../data\n",
      "[INFO|language_modeling.py:94] 2020-12-20 13:20:24,418 >> Saving features into cached file ../../data/cached_lm_GPT2Tokenizer_1024_realDonaldTrump_train.txt [took 0.007 s]\n",
      "12/20/2020 13:20:24 - INFO - filelock -   Lock 140558156480976 released on ../../data/cached_lm_GPT2Tokenizer_1024_realDonaldTrump_train.txt.lock\n",
      "[WARNING|training_args.py:372] 2020-12-20 13:20:24,421 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:372] 2020-12-20 13:20:24,423 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|trainer.py:693] 2020-12-20 13:20:24,423 >> ***** Running training *****\n",
      "[INFO|trainer.py:694] 2020-12-20 13:20:24,423 >>   Num examples = 258\n",
      "[INFO|trainer.py:695] 2020-12-20 13:20:24,424 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:696] 2020-12-20 13:20:24,424 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:697] 2020-12-20 13:20:24,424 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:698] 2020-12-20 13:20:24,424 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:699] 2020-12-20 13:20:24,424 >>   Total optimization steps = 258\n",
      "[WARNING|training_args.py:372] 2020-12-20 13:20:24,427 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "output/realDonaldTrump\n",
      "[INFO|integrations.py:344] 2020-12-20 13:20:24,433 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "[WARNING|training_args.py:372] 2020-12-20 13:20:24,433 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madityagaydhani\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33moutput/realDonaldTrump\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/adityagaydhani/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/adityagaydhani/huggingface/runs/1cgbrrxn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /media/aditya/Data/MS/Fall 2020/CSCI 5525/Project/deeptweets/models/deep_tweets_prompt/wandb/run-20201220_132024-1cgbrrxn\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 20/258 [01:40<21:05,  5.32s/it]{'loss': 3.987057113647461, 'learning_rate': 4.6124031007751936e-05, 'epoch': 0.07751937984496124}\n",
      " 16%|█▌        | 40/258 [03:29<19:55,  5.48s/it]{'loss': 3.6806331634521485, 'learning_rate': 4.2248062015503877e-05, 'epoch': 0.15503875968992248}\n",
      "                                                {'loss': 3.5538658142089843, 'learning_rate': 3.837209302325582e-05, 'epoch': 0.23255813953488372}\n",
      " 31%|███       | 80/258 [07:10<15:04,  5.08s/it]{'loss': 3.464324951171875, 'learning_rate': 3.449612403100775e-05, 'epoch': 0.31007751937984496}\n",
      " 39%|███▉      | 100/258 [08:53<14:49,  5.63s/it]{'loss': 3.373027038574219, 'learning_rate': 3.062015503875969e-05, 'epoch': 0.3875968992248062}\n",
      " 47%|████▋     | 120/258 [10:31<11:05,  4.82s/it]{'loss': 3.301316833496094, 'learning_rate': 2.674418604651163e-05, 'epoch': 0.46511627906976744}\n",
      " 54%|█████▍    | 140/258 [12:12<10:17,  5.24s/it]{'loss': 3.21280517578125, 'learning_rate': 2.2868217054263565e-05, 'epoch': 0.5426356589147286}\n",
      " 62%|██████▏   | 160/258 [13:52<08:02,  4.92s/it]{'loss': 3.197285461425781, 'learning_rate': 1.8992248062015506e-05, 'epoch': 0.6201550387596899}\n",
      " 70%|██████▉   | 180/258 [15:34<06:33,  5.05s/it]{'loss': 3.1716827392578124, 'learning_rate': 1.5116279069767441e-05, 'epoch': 0.6976744186046512}\n",
      " 78%|███████▊  | 200/258 [17:22<05:39,  5.85s/it]{'loss': 3.2307342529296874, 'learning_rate': 1.1240310077519382e-05, 'epoch': 0.7751937984496124}\n",
      " 85%|████████▌ | 220/258 [19:05<03:12,  5.07s/it]{'loss': 3.117352294921875, 'learning_rate': 7.364341085271319e-06, 'epoch': 0.8527131782945736}\n",
      " 93%|█████████▎| 240/258 [20:47<01:29,  5.00s/it]{'loss': 3.118585205078125, 'learning_rate': 3.488372093023256e-06, 'epoch': 0.9302325581395349}\n",
      "100%|██████████| 258/258 [22:18<00:00,  5.15s/it][INFO|trainer.py:829] 2020-12-20 13:42:44,429 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 258/258 [22:18<00:00,  5.15s/it]{'epoch': 1.0}\n",
      "100%|██████████| 258/258 [22:18<00:00,  5.19s/it]\n",
      "[INFO|trainer.py:1222] 2020-12-20 13:42:44,436 >> Saving model checkpoint to output/realDonaldTrump\n",
      "[INFO|configuration_utils.py:282] 2020-12-20 13:42:44,439 >> Configuration saved in output/realDonaldTrump/config.json\n",
      "[INFO|modeling_utils.py:740] 2020-12-20 13:42:47,016 >> Model weights saved in output/realDonaldTrump/pytorch_model.bin\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 24316\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /media/aditya/Data/MS/Fall 2020/CSCI 5525/Project/deeptweets/models/deep_tweets_prompt/wandb/run-20201220_132024-1cgbrrxn/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /media/aditya/Data/MS/Fall 2020/CSCI 5525/Project/deeptweets/models/deep_tweets_prompt/wandb/run-20201220_132024-1cgbrrxn/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                      train/loss 3.11859\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                     train/epoch 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                           _step 258\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                        _runtime 1343\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                      _timestamp 1608493367\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                train/total_flos 197256010530816\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▆▅▄▃▂▂▂▁▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/learning_rate █▇▇▆▅▅▄▄▃▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/epoch ▁▂▂▃▃▄▅▅▆▆▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▂▂▃▃▄▅▅▆▆▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▂▂▃▄▄▅▅▆▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▂▂▃▄▄▅▅▆▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33moutput/realDonaldTrump\u001b[0m: \u001b[34mhttps://wandb.ai/adityagaydhani/huggingface/runs/1cgbrrxn\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python ../../scripts/run_language_modeling.py \\\n",
    "    --output_dir=output/$handle \\\n",
    "    --overwrite_output_dir \\\n",
    "    --overwrite_cache \\\n",
    "    --model_type=gpt2 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --do_train --train_data_file=../../data/$handle\\_train.txt \\\n",
    "    --logging_steps 20 \\\n",
    "    --per_gpu_train_batch_size 1 \\\n",
    "    --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "viSAJ7EE7g1T"
   },
   "source": [
    "## Generate new tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uQRNMedK7g1T"
   },
   "outputs": [],
   "source": [
    "SENTENCES = [\"I think that\",\n",
    "             \"I like\",\n",
    "             \"I don't like\",\n",
    "             \"I want\",\n",
    "             \"My dream is\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EkD5CRkn7g1Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12/20/2020 13:55:50 - WARNING - __main__ -   device: cpu, n_gpu: 0, 16-bits training: False', \"12/20/2020 13:55:53 - INFO - __main__ -   Namespace(device=device(type='cpu'), fp16=False, k=0, length=160, model_name_or_path='output/realDonaldTrump', model_type='gpt2', n_gpu=0, no_cuda=False, num_return_sequences=3, p=0.95, padding_text='', prefix='', prompt='<|endoftext|>I think that', repetition_penalty=1.0, seed=2314785581, stop_token=None, temperature=1.0, xlm_language='')\", 'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.', '=== GENERATED SEQUENCE 1 ===', \"<|endoftext|>I think that politics is a match made in heaven. I don't want this country to look like the UK or France, I want it to look like Italy, Ireland, and the USA. There are TWO major parties.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\", '=== GENERATED SEQUENCE 2 ===', '<|endoftext|>I think that makes for a very good case for fixing the system: @Limbaugh didn’t fake up the case, but tried, and failed, to say he did so incorrectly (by hiding the facts). Thanks to Brooks for the great remarks on Biden. I wonder whether I’ll see the same level of enthusiasm we got from all the major sports – and Trump. See you in Wisconsin!<|endoftext|>', '=== GENERATED SEQUENCE 3 ===', '<|endoftext|>I think that the total number of terrorists and terrorist want to be factored in, however well prepared and willing they may be to slaughter Americans, will always be MUCH higher than what we get at the Donald Trump level. I would take more power or funding from @siriano. Thank you!<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>', '12/20/2020 13:55:58 - INFO - wandb.sdk.internal.internal -   Internal process exited']\n",
      "\n",
      "Start of sentence: I think that\n",
      "* Generated #1: I think that politics is a match made in heaven. I don't want this country to look like the UK or France, I want it to look like Italy, Ireland, and the USA. There are TWO major parties.\n",
      "* Generated #2: I think that makes for a very good case for fixing the system: @Limbaugh didn’t fake up the case, but tried, and failed, to say he did so incorrectly (by hiding the facts). Thanks to Brooks for the great remarks on Biden. I wonder whether I’ll see the same level of enthusiasm we got from all the major sports – and Trump. See you in Wisconsin!\n",
      "* Generated #3: I think that the total number of terrorists and terrorist want to be factored in, however well prepared and willing they may be to slaughter Americans, will always be MUCH higher than what we get at the Donald Trump level. I would take more power or funding from @siriano. Thank you!\n",
      "['12/20/2020 13:56:00 - WARNING - __main__ -   device: cpu, n_gpu: 0, 16-bits training: False', \"12/20/2020 13:56:04 - INFO - __main__ -   Namespace(device=device(type='cpu'), fp16=False, k=0, length=160, model_name_or_path='output/realDonaldTrump', model_type='gpt2', n_gpu=0, no_cuda=False, num_return_sequences=3, p=0.95, padding_text='', prefix='', prompt='<|endoftext|>I like', repetition_penalty=1.0, seed=2314785581, stop_token=None, temperature=1.0, xlm_language='')\", 'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.', '=== GENERATED SEQUENCE 1 ===', '<|endoftext|>I like politics. I enjoy it. I see it as our best chance to change politics for the better.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>', '=== GENERATED SEQUENCE 2 ===', '<|endoftext|>I like to announce how pleased I am to have had a wonderful @Limbaugh17 meeting with the folks of @TheRealJimInnes2! We will soon be on the WAY together! #ITAF<|endoftext|>', '=== GENERATED SEQUENCE 3 ===', '<|endoftext|>I like the total surprise it will get!<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>', '12/20/2020 13:56:06 - INFO - wandb.sdk.internal.internal -   Internal process exited']\n",
      "\n",
      "Start of sentence: I like\n",
      "* Generated #1: I like politics. I enjoy it. I see it as our best chance to change politics for the better.\n",
      "* Generated #2: I like to announce how pleased I am to have had a wonderful @Limbaugh17 meeting with the folks of @TheRealJimInnes2! We will soon be on the WAY together! #ITAF\n",
      "* Generated #3: I like the total surprise it will get!\n",
      "['12/20/2020 13:56:08 - WARNING - __main__ -   device: cpu, n_gpu: 0, 16-bits training: False', '12/20/2020 13:56:12 - INFO - __main__ -   Namespace(device=device(type=\\'cpu\\'), fp16=False, k=0, length=160, model_name_or_path=\\'output/realDonaldTrump\\', model_type=\\'gpt2\\', n_gpu=0, no_cuda=False, num_return_sequences=3, p=0.95, padding_text=\\'\\', prefix=\\'\\', prompt=\"<|endoftext|>I don\\'t like\", repetition_penalty=1.0, seed=2314785581, stop_token=None, temperature=1.0, xlm_language=\\'\\')', 'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.', '=== GENERATED SEQUENCE 1 ===', \"<|endoftext|>I don't like politics. I don't like Jim Crow laws. I don't like party politics. Then again, Jim Crow on many levels of VETERANS!<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\", '=== GENERATED SEQUENCE 2 ===', \"<|endoftext|>I don't like to announce how bad my case is. When I sign off on this I didn't mean for it to get this far, but, rather thanks to everyone involved. No wonder my lawyers are now saying that my fighting is also over!<|endoftext|>\", '=== GENERATED SEQUENCE 3 ===', \"<|endoftext|>I don't like running for office anymore. So I want to be Governor of Pennsylvania, for Justice, and for the people of Pennsylvania.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\", '12/20/2020 13:56:15 - INFO - wandb.sdk.internal.internal -   Internal process exited']\n",
      "\n",
      "Start of sentence: I don't like\n",
      "* Generated #1: I don't like politics. I don't like Jim Crow laws. I don't like party politics. Then again, Jim Crow on many levels of VETERANS!\n",
      "* Generated #2: I don't like to announce how bad my case is. When I sign off on this I didn't mean for it to get this far, but, rather thanks to everyone involved. No wonder my lawyers are now saying that my fighting is also over!\n",
      "* Generated #3: I don't like running for office anymore. So I want to be Governor of Pennsylvania, for Justice, and for the people of Pennsylvania.\n",
      "['12/20/2020 13:56:17 - WARNING - __main__ -   device: cpu, n_gpu: 0, 16-bits training: False', \"12/20/2020 13:56:21 - INFO - __main__ -   Namespace(device=device(type='cpu'), fp16=False, k=0, length=160, model_name_or_path='output/realDonaldTrump', model_type='gpt2', n_gpu=0, no_cuda=False, num_return_sequences=3, p=0.95, padding_text='', prefix='', prompt='<|endoftext|>I want', repetition_penalty=1.0, seed=2314785581, stop_token=None, temperature=1.0, xlm_language='')\", 'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.', '=== GENERATED SEQUENCE 1 ===', '<|endoftext|>I want to thank Arizona Congressman Larry Joe for standing with me all day this morning. With honor and words of credit, I am lowering taxes in Arizona by nearly 20%. He is an advocate for the unemployed, the sick, low-wage workers, and the hardworking man who must protect our most vulnerable & endangered American citizens. I am fighting for our neighbors, our businesses, and America First!<|endoftext|>', '=== GENERATED SEQUENCE 2 ===', '<|endoftext|>I want to announce how pleased I am to be here in Arizona! Thank you! #AZ5076<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>', '=== GENERATED SEQUENCE 3 ===', '<|endoftext|>I want the total destruction of all Democracy & Media to be factored in, for Justice & Peace!<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>', '12/20/2020 13:56:25 - INFO - wandb.sdk.internal.internal -   Internal process exited']\n",
      "\n",
      "Start of sentence: I want\n",
      "* Generated #1: I want to thank Arizona Congressman Larry Joe for standing with me all day this morning. With honor and words of credit, I am lowering taxes in Arizona by nearly 20%. He is an advocate for the unemployed, the sick, low-wage workers, and the hardworking man who must protect our most vulnerable & endangered American citizens. I am fighting for our neighbors, our businesses, and America First!\n",
      "* Generated #2: I want to announce how pleased I am to be here in Arizona! Thank you! #AZ5076\n",
      "* Generated #3: I want the total destruction of all Democracy & Media to be factored in, for Justice & Peace!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12/20/2020 13:56:28 - WARNING - __main__ -   device: cpu, n_gpu: 0, 16-bits training: False', \"12/20/2020 13:56:31 - INFO - __main__ -   Namespace(device=device(type='cpu'), fp16=False, k=0, length=160, model_name_or_path='output/realDonaldTrump', model_type='gpt2', n_gpu=0, no_cuda=False, num_return_sequences=3, p=0.95, padding_text='', prefix='', prompt='<|endoftext|>My dream is', repetition_penalty=1.0, seed=2314785581, stop_token=None, temperature=1.0, xlm_language='')\", 'Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.', '=== GENERATED SEQUENCE 1 ===', '<|endoftext|>My dream is that Asiana, plus Canada, see those failed states we win and they look on our state-by-state polling numbers of VOTER FORCE. We will never forgive China for allowing your vote.<|endoftext|>', '=== GENERATED SEQUENCE 2 ===', \"<|endoftext|>My dream is to raise 800,000 dollars for a successful business with one of America's World Famous Listens!<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\", '=== GENERATED SEQUENCE 3 ===', '<|endoftext|>My dream is Free Europe!<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>', '12/20/2020 13:56:34 - INFO - wandb.sdk.internal.internal -   Internal process exited']\n",
      "\n",
      "Start of sentence: My dream is\n",
      "* Generated #1: My dream is that Asiana, plus Canada, see those failed states we win and they look on our state-by-state polling numbers of VOTER FORCE. We will never forgive China for allowing your vote.\n",
      "* Generated #2: My dream is to raise 800,000 dollars for a successful business with one of America's World Famous Listens!\n",
      "* Generated #3: My dream is Free Europe!\n"
     ]
    }
   ],
   "source": [
    "seed = random.randint(0, 2**32-1)\n",
    "examples = []\n",
    "num_return_sequences = 3\n",
    "\n",
    "for start in SENTENCES:\n",
    "    val = !python ../../run_generation.py \\\n",
    "        --model_type gpt2 \\\n",
    "        --model_name_or_path output/$handle \\\n",
    "        --length 160 \\\n",
    "        --num_return_sequences $num_return_sequences \\\n",
    "        --temperature 1 \\\n",
    "        --p 0.95 \\\n",
    "        --seed $seed \\\n",
    "        --prompt {'\"<|endoftext|>' + start + '\"'}\n",
    "    generated = [val[-2*(k+1)] for k in range(num_return_sequences)[::-1]]\n",
    "    print(f'\\nStart of sentence: {start}')\n",
    "    for i, g in enumerate(generated):\n",
    "        g = g.replace('<|endoftext|>', '')\n",
    "        print(f'* Generated #{i+1}: {g}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "huggingtweets-dev.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
