{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPT-2 implementation is adapted from the HuggingFace library: https://huggingface.co/gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GlLlFGsS7g0i"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madityagaydhani\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qaSI1NZtMn3G"
   },
   "outputs": [],
   "source": [
    "handle = 'realDonaldTrump' # Change handle to JoeBiden for training the model on Joe Biden's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pw3L46xzNFGV"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'../../data/{handle}.csv')\n",
    "my_tweets = df['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LIutO2HNvuRg"
   },
   "outputs": [],
   "source": [
    "def make_dataset(dataset, epochs):\n",
    "    total_text = '<|endoftext|>'\n",
    "    tweets = [t for t in dataset]\n",
    "    for _ in range(epochs):\n",
    "        random.shuffle(tweets)\n",
    "        total_text += '<|endoftext|>'.join(tweets) + '<|endoftext|>'\n",
    "    return total_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4OjOW_4x7g1H"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 4\n",
    "\n",
    "with open(f'../../data/{handle}_train.txt', 'w') as f:\n",
    "    data = make_dataset(my_tweets, EPOCHS)\n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X4LWV56z7g1Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-20 16:02:05.302737: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "12/20/2020 16:02:08 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "12/20/2020 16:02:08 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='output/realDonaldTrump', overwrite_output_dir=True, do_train=True, do_eval=False, do_predict=False, model_parallel=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=1, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Dec20_16-02-08_aditya-XPS-15-9570', logging_first_step=False, logging_steps=20, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=20, dataloader_num_workers=0, past_index=-1, run_name='output/realDonaldTrump', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, fp16_backend='auto', sharded_ddp=False)\n",
      "[INFO|configuration_utils.py:431] 2020-12-20 16:02:08,364 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /home/aditya/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "[INFO|configuration_utils.py:467] 2020-12-20 16:02:08,366 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:431] 2020-12-20 16:02:09,524 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /home/aditya/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "[INFO|configuration_utils.py:467] 2020-12-20 16:02:09,525 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1802] 2020-12-20 16:02:09,993 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /home/aditya/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "[INFO|tokenization_utils_base.py:1802] 2020-12-20 16:02:09,993 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /home/aditya/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1802] 2020-12-20 16:02:09,993 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /home/aditya/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "/media/aditya/Data/MS/Fall 2020/CSCI 5525/Project/deep_tweets_workspace/lib/python3.6/site-packages/transformers/models/auto/modeling_auto.py:890: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "[INFO|modeling_utils.py:1024] 2020-12-20 16:02:10,212 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /home/aditya/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
      "[INFO|modeling_utils.py:1140] 2020-12-20 16:02:13,865 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:1149] 2020-12-20 16:02:13,865 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "/media/aditya/Data/MS/Fall 2020/CSCI 5525/Project/deep_tweets_workspace/lib/python3.6/site-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n",
      "12/20/2020 16:02:13 - INFO - filelock -   Lock 139719851905320 acquired on ../../data/cached_lm_GPT2TokenizerFast_1024_realDonaldTrump_train.txt.lock\n",
      "[INFO|language_modeling.py:88] 2020-12-20 16:02:13,867 >> Creating features from dataset file at ../../data\n",
      "[WARNING|tokenization_utils_base.py:3233] 2020-12-20 16:02:14,439 >> Token indices sequence length is longer than the specified maximum sequence length for this model (264913 > 1024). Running this sequence through the model will result in indexing errors\n",
      "[INFO|language_modeling.py:108] 2020-12-20 16:02:14,638 >> Saving features into cached file ../../data/cached_lm_GPT2TokenizerFast_1024_realDonaldTrump_train.txt [took 0.008 s]\n",
      "12/20/2020 16:02:14 - INFO - filelock -   Lock 139719851905320 released on ../../data/cached_lm_GPT2TokenizerFast_1024_realDonaldTrump_train.txt.lock\n",
      "[WARNING|training_args.py:423] 2020-12-20 16:02:14,644 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:423] 2020-12-20 16:02:14,646 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|trainer.py:703] 2020-12-20 16:02:14,646 >> ***** Running training *****\n",
      "[INFO|trainer.py:704] 2020-12-20 16:02:14,646 >>   Num examples = 258\n",
      "[INFO|trainer.py:705] 2020-12-20 16:02:14,646 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:706] 2020-12-20 16:02:14,646 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:707] 2020-12-20 16:02:14,646 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:708] 2020-12-20 16:02:14,646 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:709] 2020-12-20 16:02:14,646 >>   Total optimization steps = 258\n",
      "[WARNING|training_args.py:423] 2020-12-20 16:02:14,649 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/realDonaldTrump\n",
      "[INFO|integrations.py:371] 2020-12-20 16:02:14,658 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "[WARNING|training_args.py:423] 2020-12-20 16:02:14,659 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madityagaydhani\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.12\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33moutput/realDonaldTrump\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/adityagaydhani/huggingface\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/adityagaydhani/huggingface/runs/xmxvdckb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /media/aditya/Data/MS/Fall 2020/CSCI 5525/Project/deep_tweets_workspace/deeptweets/models/GPT2/wandb/run-20201220_160215-xmxvdckb\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "  8%|▊         | 20/258 [01:36<18:58,  4.78s/it]{'loss': 3.9922882080078126, 'learning_rate': 4.6124031007751936e-05, 'epoch': 0.07751937984496124}\n",
      " 16%|█▌        | 40/258 [03:10<17:08,  4.72s/it]{'loss': 3.648388671875, 'learning_rate': 4.2248062015503877e-05, 'epoch': 0.15503875968992248}\n",
      " 23%|██▎       | 60/258 [04:46<15:40,  4.75s/it]{'loss': 3.534408950805664, 'learning_rate': 3.837209302325582e-05, 'epoch': 0.23255813953488372}\n",
      " 31%|███       | 80/258 [06:21<14:13,  4.79s/it]{'loss': 3.4398590087890626, 'learning_rate': 3.449612403100775e-05, 'epoch': 0.31007751937984496}\n",
      " 39%|███▉      | 100/258 [07:59<14:04,  5.34s/it]{'loss': 3.3521408081054687, 'learning_rate': 3.062015503875969e-05, 'epoch': 0.3875968992248062}\n",
      " 47%|████▋     | 120/258 [09:45<12:03,  5.24s/it]{'loss': 3.3005859375, 'learning_rate': 2.674418604651163e-05, 'epoch': 0.46511627906976744}\n",
      " 54%|█████▍    | 140/258 [11:22<09:34,  4.87s/it]{'loss': 3.2370304107666015, 'learning_rate': 2.2868217054263565e-05, 'epoch': 0.5426356589147286}\n",
      " 62%|██████▏   | 160/258 [12:59<07:54,  4.84s/it]92248062015506e-05, 'epoch': 0.6201550387596899}\n",
      " 70%|██████▉   | 180/258 [14:36<06:18,  4.85s/it]{'loss': 3.209954833984375, 'learning_rate': 1.5116279069767441e-05, 'epoch': 0.6976744186046512}\n",
      " 78%|███████▊  | 200/258 [16:15<05:12,  5.40s/it]{'loss': 3.2073490142822267, 'learning_rate': 1.1240310077519382e-05, 'epoch': 0.7751937984496124}\n",
      " 85%|████████▌ | 220/258 [17:51<03:03,  4.83s/it]{'loss': 3.0966207504272463, 'learning_rate': 7.364341085271319e-06, 'epoch': 0.8527131782945736}\n",
      "                                                 {'loss': 3.120669364929199, 'learning_rate': 3.488372093023256e-06, 'epoch': 0.9302325581395349}\n",
      "100%|██████████| 258/258 [20:56<00:00,  4.85s/it][INFO|trainer.py:862] 2020-12-20 16:23:12,894 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'epoch': 1.0}                                   \n",
      "100%|██████████| 258/258 [20:56<00:00,  4.87s/it]\n",
      "[INFO|trainer.py:1226] 2020-12-20 16:23:12,898 >> Saving model checkpoint to output/realDonaldTrump\n",
      "[INFO|configuration_utils.py:289] 2020-12-20 16:23:12,902 >> Configuration saved in output/realDonaldTrump/config.json\n",
      "[INFO|modeling_utils.py:814] 2020-12-20 16:23:15,377 >> Model weights saved in output/realDonaldTrump/pytorch_model.bin\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 10076\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /media/aditya/Data/MS/Fall 2020/CSCI 5525/Project/deep_tweets_workspace/deeptweets/models/GPT2/wandb/run-20201220_160215-xmxvdckb/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /media/aditya/Data/MS/Fall 2020/CSCI 5525/Project/deep_tweets_workspace/deeptweets/models/GPT2/wandb/run-20201220_160215-xmxvdckb/logs/debug-internal.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                      train/loss 3.12067\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                             train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                     train/epoch 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                           _step 258\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                        _runtime 1260\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                      _timestamp 1608502995\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                train/total_flos 197256010530816\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/loss █▅▄▄▃▃▂▂▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/learning_rate █▇▇▆▅▅▄▄▃▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train/epoch ▁▂▂▃▃▄▅▅▆▆▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                 _step ▁▂▂▃▃▄▅▅▆▆▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              _runtime ▁▂▂▃▄▄▅▅▆▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            _timestamp ▁▂▂▃▄▄▅▅▆▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      train/total_flos ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33moutput/realDonaldTrump\u001b[0m: \u001b[34mhttps://wandb.ai/adityagaydhani/huggingface/runs/xmxvdckb\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python ../../scripts/run_language_modeling.py \\\n",
    "    --output_dir=output/$handle \\\n",
    "    --overwrite_output_dir \\\n",
    "    --overwrite_cache \\\n",
    "    --model_type=gpt2 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --do_train --train_data_file=../../data/$handle\\_train.txt \\\n",
    "    --logging_steps 20 \\\n",
    "    --per_gpu_train_batch_size 1 \\\n",
    "    --num_train_epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "viSAJ7EE7g1T"
   },
   "source": [
    "## Generate new tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uQRNMedK7g1T"
   },
   "outputs": [],
   "source": [
    "SENTENCES = [\"I think that\",\n",
    "             \"I like\",\n",
    "             \"I don't like\",\n",
    "             \"I want\",\n",
    "             \"My dream is\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EkD5CRkn7g1Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of sentence: I think that\n",
      "* Generated #1: I think that giving credit where credit is due, like the likes of Schumer, is unprecedented. These are the lunatics who went to steal our Democracy. Radical Left Democrats with absolutely zero pre-existing conditions!\n",
      "* Generated #2: I think that everything went according to plan. No one felt compelled to make any concessions. They all got a terrific surprise from our National Security...\n",
      "* Generated #3: I think that your fault for this election is not your fault, but the fact that he didn't win. Many others have done the same. His small group of fools and pandering, though very good, don’t give a damn, he just ruined your credibility.\n",
      "\n",
      "Start of sentence: I like\n",
      "* Generated #1: I like to say I've had a GREAT MAN. He called me \"Bored With Brothers,\" and he was very happy to help me. I am proud to say he is a local hero, and I will never forget him!\n",
      "* Generated #2: I like everything about my daughter. She loved Parkour, she loves Harney, she loves making a difference... But we got along great! I saw her very much. She doesn't care about your politics or guns. Be brave and stand up for us all!\n",
      "* Generated #3: I like your Outback Country, but miss your guts and love you so much more than I did. I want to thank President Bill Clinton and Justice & FIRE for allowing my father, Joe, and David to teach me the values he has to lead our Country! #MakeAmericaGreatAgain\n",
      "\n",
      "Start of sentence: I don't like\n",
      "* Generated #1: I don't like giving credit where credit is due, because the Obamas lost so many jobs. These are job losses, but being left in Washington can soon mean that Democrats with absolutely zero pre-existing conditions lose their jobs and long waits for care, long waits in Medicare or even C-SPAN to pay for your care, long waits for CHIPT A, and much more. I can lose a family member in an attack...\n",
      "* Generated #2: I don't like everything except my daughter. Sleepy Joe. When she asks for a raise, she never delivers.\n",
      "* Generated #3: I don't like your Outback town halls. There were many negative and inaccurate lies! Don't let their ignorant and self sustaining fans go, because they will hurt your election and your credibility!\n",
      "\n",
      "Start of sentence: I want\n",
      "* Generated #1: I want to thank all of the members of our Great Rally Team for their commitment to the Second Amendment. We will be leaving Early tonight in Baltimore, Pennsylvania, and with you, we will NEVER, ever, EVER give up our Second Amendment, our Constitutional rights, or your right to defend and defend your home and family. Now is the time to recover. The clock is ticking. #MAGA\n",
      "* Generated #2: I want everything done, whether done in the Park or via the Court. MAKE AMERICA GREAT AGAIN! #MakeAmericaGreatAgain\n",
      "* Generated #3: I want to thank the American Red Cross and the many thousands of the other Federal State and Local Law Enforcement who are helping us fight, saving and providing medical care to our incredible, and brave, people. We are grateful for the people of Ohio who used to stand against these plain, filthy, lawless criminals and vowed to do the same for all American heroes. So much love to Bryan!\n",
      "\n",
      "Start of sentence: My dream is\n",
      "* Generated #1: \n",
      "* Generated #2: Now, am I the only American who can cure America of China?\n",
      "* Generated #3: My dream is to be a Philadelphia City Senator. Thank you!\n"
     ]
    }
   ],
   "source": [
    "seed = random.randint(0, 2**32-1)\n",
    "examples = []\n",
    "num_return_sequences = 3\n",
    "\n",
    "for start in SENTENCES:\n",
    "    val = !python ../../scripts/run_generation.py \\\n",
    "        --model_type gpt2 \\\n",
    "        --model_name_or_path output/$handle \\\n",
    "        --length 160 \\\n",
    "        --num_return_sequences $num_return_sequences \\\n",
    "        --temperature 1 \\\n",
    "        --p 0.95 \\\n",
    "        --seed $seed \\\n",
    "        --prompt {'\"<|endoftext|>' + start + '\"'}\n",
    "    generated = [val[-2*(k+1)] for k in range(num_return_sequences)[::-1]]\n",
    "    print(f'\\nStart of sentence: {start}')\n",
    "    for i, g in enumerate(generated):\n",
    "        g = g.replace('<|endoftext|>', '')\n",
    "        print(f'* Generated #{i+1}: {g}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "huggingtweets-dev.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
